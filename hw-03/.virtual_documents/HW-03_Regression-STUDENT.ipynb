





























# Put your code here
import pandas as pd
red = pd.read_csv("winequality-red.csv", sep  = ';')
white = pd.read_csv("winequality-white.csv", sep = ';')
print(red.head())
print(red.tail())
print(white.head())
print(white.tail())





# Put your code here
import seaborn as sns
import matplotlib.pyplot as plt
corr = red.corr()
fig, ax = plt.subplots(1,1, figsize = (10,10) )
heatmap = sns.heatmap(corr, annot = True)
plt.tight_layout()
heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=70) 
#Any higher rotation and they start to overlap





# Put your code here
# Put your code here
import seaborn as sns
import matplotlib.pyplot as plt
corr = white.corr()
fig, ax = plt.subplots(1,1, figsize = (10,10) )
heatmap = sns.heatmap(corr, annot = True)
plt.tight_layout()
heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=70) 
#Any higher rotation and they start to overlap



#Differences in Features:
    # Both appear to have strong correlation between Free and total Sulfur Dioxide
    #Biggest difference is correlations for density.
    #Red wine has very strong postiive correlation between denisty and citric acid and citric acid and fixed acidity
    #White wine has strong negative correlation between alcohol and density and strong positive correlation between
    #residual sugar and density.
    #Annot = True used to determine this, but looks ugly so not included in final heatmaps.








# Put your code here
import statsmodels.api as sm
import numpy as np
wine = pd.concat([red, white])
y_ary = np.array(wine['fixed acidity'])
x_ary = np.array(wine['citric acid'])
x_ary = sm.add_constant(x_ary)
model = sm.OLS(y_ary, x_ary)
results = model.fit()
print(results.summary())
print(results.params)
print(results.pvalues)











# Put your code here
y_red = np.array(red['fixed acidity'])
x_red = np.array(red['citric acid'])
x_red = sm.add_constant(x_red)
model_red = sm.OLS(y_red, x_red)
results_red = model_red.fit()
print(results_red.summary())
print(results_red.params)





# Put your code here
y_white = np.array(white['fixed acidity'])
x_white = np.array(white['citric acid'])
x_white = sm.add_constant(x_white)
model_white = sm.OLS(y_white, x_white)
results_white = model_white.fit()
print(results_white.summary())
print(results_white.params)











# Put your code here
#Blue is for white wine since being white tends to not show up well.
import matplotlib.pyplot as plt
import numpy as np
def plot_line(slope, intercept, xmin, xmax, color):
    xline = np.array([xmin,xmax])
    yline = slope*xline+intercept
    plt.plot(xline,yline,color)
fig, ax = plt.subplots(1,1, figsize = (10,10) )
plt.plot(red['citric acid'], red['fixed acidity'], 'o', color = "red", label = "Red Wine")
plt.plot(white['citric acid'], white['fixed acidity'], 'o', color = "blue", label = "White Wine")
plot_line(results_red.params[1], results_red.params[0], 0, 1.75, "red")
plot_line(results_white.params[1], results_white.params[0], 0, 1.75, "blue")
plot_line(results.params[1], results.params[0], 0, 1.75, "black")
plt.xlabel("Citric Acid")
plt.ylabel("Fixed Acidity")
plt.title("Fixed Acidity vs Citric Acid")
plt.legend()











# Put your code here
y = np.array(red['quality'])
x = red[['volatile acidity', 'citric acid', 'alcohol']]
x = sm.add_constant(x)
model_multi = sm.OLS(y, x)
results_multi = model_multi.fit()
print(results_multi.summary())
print(results_multi.params)











# Put your code here
#I checked the array and determined which one were chemicals and then added them to the ones we were already investigating.
y_chem = np.array(red['quality'])
x_chem = red[['volatile acidity', 'citric acid', 'alcohol', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates']]
x_chem = sm.add_constant(x_chem)
model = sm.OLS(y_chem, x_chem)
results = model.fit()
print(results.summary())
print(results.params)























# Put your code here
red['color'] = 1
white['color'] = 0
wine2 = pd.concat([red, white])
wine3 = wine2
labels = wine2['color']
features = wine3.drop(columns = ['quality', 'color'])






# Put your code here
from sklearn.model_selection import train_test_split

train_vectors, test_vectors, train_labels, test_labels = train_test_split(features,
                                                                          labels, test_size=0.25, random_state=0)
print("Train Vector Shape" , train_vectors.shape)
print("Test Vector Shape" , test_vectors.shape)
print("Train Labels Shape" , train_labels.shape)
print("Test Labels Shape" , test_labels.shape)





# Put your code here
logit_model = sm.Logit(train_labels, sm.add_constant(train_vectors))
result = logit_model.fit()
print(result.summary())





# Put your code here
from sklearn import metrics
test_vectors = sm.add_constant(test_vectors) 
array = result.predict(test_vectors)
rounded = array.round()
print(rounded)

print("Accuracy of Our Model:", round(100 * metrics.accuracy_score(rounded, test_labels),3), '%')




















